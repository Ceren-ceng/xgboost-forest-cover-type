
Bu projede, `covtype.csv` veri seti kullanÄ±larak **XGBoost (Extreme Gradient Boosting)** algoritmasÄ±yla orman Ã¶rtÃ¼sÃ¼ tÃ¼rleri sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸtÄ±r. Boosting mantÄ±ÄŸÄ±, XGBoostâ€™un matematiksel yapÄ±sÄ±, veri Ã¶n iÅŸleme sÃ¼reci ve model baÅŸarÄ±mÄ± detaylÄ± ÅŸekilde aÃ§Ä±klanmÄ±ÅŸtÄ±r.

---

## ğŸ“Œ 1. Ensemble Learning ve Boosting Nedir?

**Ensemble Learning**, birden fazla zayÄ±f modelin birleÅŸerek daha gÃ¼Ã§lÃ¼ bir tahmin modeli oluÅŸturmasÄ±dÄ±r. Boosting ise bu yaklaÅŸÄ±mÄ±n art arda hatalarÄ± dÃ¼zelterek ilerleyen versiyonudur.

### Ensemble Ã‡eÅŸitleri:
- **Bagging**: Paralel modeller (Ã¶rn. Random Forest)
- **Boosting**: SÄ±ralÄ± modeller (Ã¶rn. AdaBoost, XGBoost)
- **Stacking**: FarklÄ± modellerin Ã§Ä±ktÄ±larÄ± ile ikinci bir model

---

## âš¡ 2. XGBoost Nedir?

**XGBoost**, hata fonksiyonunun (loss) gradyanÄ±na dayalÄ± olarak Ã§alÄ±ÅŸan, optimize edilmiÅŸ bir boosting algoritmasÄ±dÄ±r.

### AvantajlarÄ±:
- Ã‡ok hÄ±zlÄ± ve paralel iÅŸlem destekli (C++ tabanlÄ±)
- Overfittingâ€™e karÅŸÄ± L1/L2 regularization iÃ§erir
- Eksik verilerle Ã§alÄ±ÅŸabilir
- BÃ¼yÃ¼k veri setleri iÃ§in uygundur

---

## ğŸ§  3. XGBoostâ€™un Matematiksel MantÄ±ÄŸÄ±

XGBoost, her tahmin iterasyonunda hatayÄ± minimize etmeye Ã§alÄ±ÅŸÄ±r. Ana fikir:

### BaÅŸlangÄ±Ã§:
- TÃ¼m tahminler sabit bir deÄŸerden baÅŸlar:  
  \[
  \hat{y}^{(0)} = 	ext{average}(y)
  \]

### Her iterasyon iÃ§in:
1. **KayÄ±p Fonksiyonu (Loss)** belirlenir:  
   Genelde logloss veya squared error.

2. **Gradyan (g)** ve **Hessian (h)** hesaplanÄ±r:  
   \[
   g_i = rac{\partial L(y_i, \hat{y}_i)}{\partial \hat{y}_i}, \quad
   h_i = rac{\partial^2 L(y_i, \hat{y}_i)}{\partial \hat{y}_i^2}
   \]

3. Yeni aÄŸaÃ§ bu gradyanlara gÃ¶re fit edilir.  
   AÄŸaÃ§ her nodeâ€™da \( g, h \) kullanarak split gain hesaplar.

4. Yeni tahmin eklenir:
   \[
   \hat{y}^{(t)} = \hat{y}^{(t-1)} + \eta f_t(x)
   \]
   - \( \eta \): learning rate  
   - \( f_t(x) \): t. iterasyonda Ã¶ÄŸrenilen model (aÄŸaÃ§)

5. Toplam hedef:
   \[
   	ext{Obj} = \sum_i L(y_i, \hat{y}_i^{(t)}) + \sum_t \Omega(f_t)
   \]
   - \( \Omega(f_t) \): model karmaÅŸÄ±klÄ±k cezasÄ±
   - 
## Matematik KÄ±smÄ± KarÄ±ÅŸÄ±k Geldiyse

ğŸ§© AdÄ±m 1: BaÅŸlangÄ±Ã§ Tahmini YapÄ±lÄ±r
Model ilk olarak hiÃ§bir ÅŸey Ã¶ÄŸrenmeden tÃ¼m Ã¶rnekler iÃ§in aynÄ± tahmini yapar.

Mesela sÄ±nÄ±flandÄ±rma iÃ§in baÅŸlangÄ±Ã§ deÄŸeri genelde sabittir (Ã¶rneÄŸin logloss iÃ§in 0.5 olabiliyor).

Yani model baÅŸta herkese â€œbence bu sÄ±nÄ±fâ€ der ama tahminleri pek isabetli deÄŸildir.

ğŸ¯ AdÄ±m 2: Hatalar (Loss) HesaplanÄ±r
Her Ã¶rnek iÃ§in modelin ne kadar hata yaptÄ±ÄŸÄ± hesaplanÄ±r.

Hangi Ã¶rneklerde yanlÄ±ÅŸ tahmin yaptÄ±ysa, o Ã¶rneklere daha Ã§ok dikkat edilmesi gerekir.

Ama XGBoost klasik hata oranÄ±yla deÄŸil, kayÄ±p fonksiyonunun tÃ¼revi ile Ã§alÄ±ÅŸÄ±r.

ğŸ“‰ AdÄ±m 3: Gradyan ve Hessian HesaplanÄ±r
HatalarÄ±n eÄŸimini (gradyanÄ±nÄ±) ve eÄŸriliÄŸini (hessian) hesaplar.

Bu ne demek?

Gradyan: â€œTahminimi ne kadar ve hangi yÃ¶nde deÄŸiÅŸtirmeliyim?â€

Hessian: â€œNe kadar hÄ±zlÄ± veya yavaÅŸ deÄŸiÅŸtirmeliyim?â€

ğŸŒ³ AdÄ±m 4: Yeni AÄŸaÃ§ Bu Hatalara GÃ¶re Ã–ÄŸretilir
Åimdi bir karar aÄŸacÄ± modeli eÄŸitilir.

AmaÃ§: Bu aÄŸacÄ±n, bir Ã¶nceki modelin en kÃ¶tÃ¼ tahmin ettiÄŸi (gradyanÄ± bÃ¼yÃ¼k olan) verileri daha iyi aÃ§Ä±klamasÄ±.

AÄŸaÃ§ her dalÄ±nda ÅŸunu sorar:

â€œBu bÃ¶lmeyi yaparsam gradyanÄ± ne kadar azaltÄ±rÄ±m?â€

â€œSplit Gainâ€ diye bir hesap yapÄ±lÄ±r: o split ne kadar faydalÄ±ysa, aÄŸaÃ§ onu yapar.

â• AdÄ±m 5: Yeni Tahminler Ã–nceki Tahmine Eklenir
Ã–ÄŸrenilen yeni aÄŸaÃ§, eski tahminlerin Ã¼stÃ¼ne eklenir:

ğ‘¦
^
(
ğ‘¡
)
=
ğ‘¦
^
(
ğ‘¡
âˆ’
1
)
+
ğœ‚
â‹…
ğ‘“
ğ‘¡
(
ğ‘¥
)
y
^
â€‹
  
(t)
 = 
y
^
â€‹
  
(tâˆ’1)
 +Î·â‹…f 
t
â€‹
 (x)
Buradaki 
ğœ‚
Î· bir learning rateâ€™tir â†’ Ne kadar dÃ¼zeltme yapacaÄŸÄ±mÄ±zÄ± belirler.

Ã‡ok bÃ¼yÃ¼kse overfit olur, Ã§ok kÃ¼Ã§Ã¼kse Ã¶ÄŸrenemez.

ğŸ” AdÄ±m 6: Bu Ä°ÅŸlem Tekrar Edilir
Bu sÃ¼reÃ§ 100 kere (ya da n_estimators kadar) tekrar eder.

Her iterasyon, bir Ã¶nceki hatalarÄ±n Ã¼stÃ¼ne yeni bir â€œdÃ¼zeltici aÄŸaÃ§â€ ekler.

Sonunda hepsi birleÅŸir ve gÃ¼Ã§lÃ¼ bir model oluÅŸturur.

ğŸ“¦ Ekstra: Model KarmaÅŸÄ±klÄ±ÄŸÄ±na Ceza Verilir
Ã‡ok fazla bÃ¶lÃ¼nme yapan, aÅŸÄ±rÄ± detaylÄ± aÄŸaÃ§lara ceza verilir (regularization).

BÃ¶ylece overfitting engellenir.

ğŸ”š Ã–zetle:
TÃ¼m Ã¶rneklere baÅŸlangÄ±Ã§ta aynÄ± tahmin yapÄ±lÄ±r.

Modelin yaptÄ±ÄŸÄ± hatalar (gradyan) hesaplanÄ±r.

Yeni bir aÄŸaÃ§, bu hatalarÄ± dÃ¼zeltmek Ã¼zere eÄŸitilir.

Yeni tahmin, eski tahmine eklenir.

Bu dÃ¶ngÃ¼ onlarca kez tekrar eder.

SonuÃ§: Her adÄ±mda hatalarÄ± dÃ¼zelterek oluÅŸmuÅŸ Ã§ok gÃ¼Ã§lÃ¼ bir model.



---

## ğŸ“Š 4. KullanÄ±lan Veri Seti: `covtype.csv`

| Ã–zellik         | AÃ§Ä±klama |
|------------------|----------|
| KayÄ±t sayÄ±sÄ±     | 581,012  |
| SÃ¼tun sayÄ±sÄ±     | 55       |
| Hedef deÄŸiÅŸken   | Cover_Type (1â€“7 arasÄ±) |
| TÃ¼r              | Ã‡oklu sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma |

### SÄ±nÄ±flar:
1. Spruce/Fir  
2. Lodgepole Pine  
3. Ponderosa Pine  
4. Cottonwood/Willow  
5. Aspen  
6. Douglas-fir  
7. Krummholz  

Veri, Coloradoâ€™daki Roosevelt Ulusal OrmanÄ±â€™ndan toplanmÄ±ÅŸtÄ±r. CoÄŸrafi ve toprak verilerine gÃ¶re bitki Ã¶rtÃ¼sÃ¼nÃ¼ tahmin etme hedeflenmiÅŸtir.

---

## ğŸ”§ 5. Veri Ã–n Ä°ÅŸleme

1. **Dengeleme**: Her sÄ±nÄ±ftan eÅŸit Ã¶rnek alÄ±narak dengeli veri seti oluÅŸturuldu.
```python
n = df["Cover_Type"].value_counts().min()
df_balanced = df.groupby("Cover_Type", group_keys=False).apply(lambda x: x.sample(n=n, random_state=42))
```

2. **SÄ±nÄ±flar 0'dan baÅŸlatÄ±ldÄ±**:
```python
y = df_balanced["Cover_Type"] - 1
```

3. **EÄŸitim-test ayrÄ±mÄ±**:
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
```

---

## ğŸ¤– 6. Model Kurulumu

```python
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=100,
    random_state=42,
    use_label_encoder=False,
    eval_metric='mlogloss'
)
xgb.fit(X_train, y_train)
```

---

## ğŸ“ˆ 7. Model PerformansÄ±

| Metrik     | DeÄŸer     |
|------------|-----------|
| Accuracy   | %85.65    |
| Precision, Recall, F1 | TÃ¼m sÄ±nÄ±flar iÃ§in detaylÄ± |

Confusion Matrix ve Feature Importance gÃ¶rsellerle desteklenmiÅŸtir.

---

## ğŸ”¬ 8. Feature Importance

```python
xgb.feature_importances_
```

GÃ¶rsel grafikle en Ã¶nemli 15 Ã¶zellik Ã§izdirildi.  
En kritik Ã¶zellik: **Elevation**. DiÄŸerleri mesafe ve Ä±ÅŸÄ±k sÃ¼tunlarÄ±.

---

## âš”ï¸ 9. AdaBoost â€“ XGBoost â€“ RandomForest KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik     | AdaBoost     | XGBoost        | Random Forest |
|-------------|--------------|----------------|----------------|
| Boosting mi? | âœ…          | âœ…             | âŒ (Bagging)   |
| Temel YÃ¶ntem | HatalÄ± Ã¶rnek aÄŸÄ±rlÄ±ÄŸÄ± | Gradyan ve 2. tÃ¼rev | Bootstrap Ã¶rnekleme |
| Performans   | Orta         | YÃ¼ksek         | Orta-YÃ¼ksek    |
| Regularization | Yok       | Var (L1/L2)     | Yok            |

---
